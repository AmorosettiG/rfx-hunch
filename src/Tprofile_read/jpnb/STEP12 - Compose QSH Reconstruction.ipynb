{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.colors as colors \n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import ipysh\n",
    "\n",
    "%aimport models.base\n",
    "\n",
    "import Hunch_utils  as Htls\n",
    "import Hunch_lsplot as Hplt\n",
    "import Hunch_tSNEplot as Hsne\n",
    "\n",
    "%aimport Dataset_QSH\n",
    "\n",
    "%aimport models.AEFIT5\n",
    "%aimport models.Compose\n",
    "\n",
    "# ipysh.Bootstrap_support.debug()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST QSH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QSH rebalanced 15 points size:  47567\n"
     ]
    }
   ],
   "source": [
    "qsh = Dataset_QSH.Dataset_QSH()\n",
    "import os\n",
    "file = ipysh.abs_builddir+'/te_db_r15_clean.npy'\n",
    "try: qsh.load(file)\n",
    "except: raise FileNotFoundError\n",
    "    \n",
    "qsh.shuffle()\n",
    "qsh.dim = None\n",
    "qsh.set_null(np.nan)\n",
    "qsh.set_normal_positive(['prel','te','tbordo','tcentro', 'Ip','NS','VT','F'])\n",
    "# qsh.set_normal_positive(['prel','te','tbordo','tcentro', 'Ip','NS','VT','F','absBr_rm','argBr_rm'])\n",
    "# qsh.unbias_mean(0.5, 'te')\n",
    "# qsh.set_normal_positive(['te'])\n",
    "# qsh.clip_values(0.1,0.6)\n",
    "# qsh.set_normal_positive(['te'])\n",
    "\n",
    "\n",
    "print(\"QSH rebalanced 15 points size: \", len(qsh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0830 09:48:35.417568 140034843486016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:504: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "Br_min, Br_max = np.nanmin(qsh['Br_rm']), np.nanmax(qsh['Br_rm'])\n",
    "def _map(xy,p,Br):\n",
    "    Br = (Br-Br_min)/(Br_max-Br_min)\n",
    "    pBr = tf.concat([p,Br], axis=0)\n",
    "    return (xy,pBr),(xy,pBr)\n",
    "# ds = qsh.tf_tuple_compose(['prel','te','tbordo~tcentro~Ip~NS~VT~F','absBt_rm~argBt_rm']).map(lambda x,y,t,s: (_map(x,y,t,s)) )\n",
    "# ds = qsh.tf_tuple_compose(['prel~te:15','Ip~NS~VT~F','absBr_rm~argBr_rm']).map(lambda x,y,z: _map(x,y,z) )\n",
    "ds = qsh.tf_tuple_compose(['prel~te:15','Ip~NS~VT~F','Br_rm']).map(lambda x,y,z: _map(x,y,z) )\n",
    "# [x for x in ds.shuffle(100).batch(1).take(1)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AEFIT5 a ready:\n",
      "AEFIT5 a ready:\n",
      "AEFIT5 a ready:\n",
      "[(None, 30), (None, 24)]\n"
     ]
    }
   ],
   "source": [
    "m1 = models.AEFIT5.AEFIT5(latent_dim=10, feature_dim=30,  dprate=0.5, scale=1, geometry=[20,20,10], beta=0.) #beta=0.001)\n",
    "m2 = models.AEFIT5.AEFIT5(latent_dim=24, feature_dim=24,  dprate=0.2, scale=1, beta=0., name='parameters', geometry=[1]) # parameters\n",
    "hm_feature_dim = m1.latent_dim + m2.latent_dim\n",
    "hm = models.AEFIT5.AEFIT5(latent_dim=10, feature_dim=hm_feature_dim, beta=0., scale=1, name='hidden', geometry=[20,80,80,50])\n",
    "h = models.Compose.Compose().set_model(hm).compose([m1,m2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7511bfc8319747419e4ac0f25f109623"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079f30a0e4194178849340b03a54c3a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694dcecdf34b471da9b07b47123ed4e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9ea67313c7424c96aa337104d01060"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_m1 = qsh.tf_tuple_compose(['prel~te:15']).map(lambda x: (x,x) )\n",
    "models.base.train_thread(m1, ds_m1, batch=100, epoch=10, callbacks=[]).control_panel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m1.save('step12_m1_compose')\n",
    "m1.load('step12_m1_compose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.trainable = False\n",
    "m2.trainable = True\n",
    "hm.trainable = True\n",
    "h.loss_weights = [0.6,0.4]\n",
    "\n",
    "# l1 = tf.keras.regularizers.l1\n",
    "# l2 = tf.keras.regularizers.l2\n",
    "# for n in [hm.inference_net, hm.generative_net]:\n",
    "#     for l in n.layers:\n",
    "#         if issubclass(type(l), tf.keras.layers.Dense):\n",
    "#             l.activity_regularizer = l1(0.01)\n",
    "            \n",
    "h.compile( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/devel/rfx/rfx-hunch/build/conf/python/site-packages/lib/python3.6/site-packages/livelossplot/core.py:15: UserWarning: livelossplot requires inline plots.\n",
      "Your current backend is: nbAgg\n",
      "Run in a Jupyter environment and execute '%matplotlib inline'.\n",
      "  warnings.warn(\"livelossplot requires inline plots.\\nYour current backend is: {}\\nRun in a Jupyter environment and execute '%matplotlib inline'.\".format(backend))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00811fd4a1d8497bbc1ac9caa1af3654"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c2a13b1f8543149c58002137a4afcf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ebd78c6d7d442bace959357ff0a20c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127438401d314445accaf1a980653144"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "     75/Unknown - 18s 244ms/step - loss: 30.8500 - output_1_loss: 15.7615 - output_2_loss: 15.0886"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread async train:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/andrea/devel/rfx/rfx-hunch/build/conf/python/site-packages/lib/python3.6/site-packages/nbmultitask.py\", line 148, in run\n",
      "    fn(*self._args,**self._kwargs)\n",
      "  File \"/home/andrea/devel/rfx/rfx-hunch/src/Tprofile_read/models/base.py\", line 249, in <lambda>\n",
      "    fn = lambda thread_print: train(model, data, **kwargs)\n",
      "  File \"/home/andrea/devel/rfx/rfx-hunch/src/Tprofile_read/models/base.py\", line 239, in train\n",
      "    history = model.fit(data, epochs=epoch, callbacks=callbacks + tensorboard_log(log_name), verbose=1)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 643, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\", line 694, in fit\n",
      "    steps_name='steps_per_epoch')\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\", line 300, in model_iteration\n",
      "    aggregator.finalize()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\", line 118, in finalize\n",
      "    self.results[0] /= self.num_samples_or_steps\n",
      "TypeError: unsupported operand type(s) for /: 'float' and 'NoneType'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import livelossplot.keras\n",
    "class PlotLossesCallback(livelossplot.keras.PlotLossesCallback):\n",
    "    def on_train_batch_begin(self, a, b): pass\n",
    "    def on_train_batch_end(self, a, b): pass\n",
    "\n",
    "# hm.beta = 1.\n",
    "# h.compile()\n",
    "models.base.train_thread(h, ds.take(10000) , epoch=100, batch=100, learning_rate=1e-3, callbacks=[PlotLossesCallback()]).control_panel()\n",
    "# h.fit(ds.batch(100), epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ v.name for v in h2.trainable_variables ]\n",
    "def plot(xy):\n",
    "    plt.figure('data')\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim(0.,.8)\n",
    "    x,y = tf.split(xy[0], num_or_size_splits=2)\n",
    "    plt.plot(x,y,'.')\n",
    "d = [x for x in ds.shuffle(100).batch(1).take(100)][0]\n",
    "d0 = d[0][0]\n",
    "d_param = ( tf.zeros_like(d[0][0]), d[0][1] ) # spengo SXR e tengo i parametri\n",
    "# y0 = h(d[0], training=False)\n",
    "y0 = h(d_param, training=False)\n",
    "y0 = tf.sigmoid(y0[0])\n",
    "plot(d0)\n",
    "plot(y0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tbordo tcentro Ip NS VT F\n",
    "xy,_ =  [x for x in ds.batch(1).take(1)][0]\n",
    "par = xy[2]\n",
    "\n",
    "l,_ = h.encode(xy, training=False)\n",
    "XY  = h.decode(l, training=False, apply_sigmoid=True) \n",
    "PAR = XY[2]\n",
    "\n",
    "print( list(zip(par, PAR)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure('test_curve',figsize=(18, 6))\n",
    "plt.clf()\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)    \n",
    "# ax1.set_xlim(-2.,2.)\n",
    "ax2.set_ylim(0.,1.)\n",
    "\n",
    "# sx = []\n",
    "# sy = []\n",
    "# for xy in ds.batch(1).take(1000):\n",
    "#     xy,_ = xy\n",
    "#     x,y = tf.split(xy[0],2, axis=1)\n",
    "#     me,_  = h.encode(xy, training=False)\n",
    "#     gpt = me[0].numpy()\n",
    "#     #ax1.scatter(gpt[0],gpt[1])\n",
    "#     sx.append(gpt[0])\n",
    "#     sy.append(gpt[1])\n",
    "\n",
    "# ax1.scatter(sx,sy)\n",
    "    \n",
    "for xy in ds.shuffle(100).batch(1).take(1):    \n",
    "    xy,_ = xy\n",
    "    x,y = tf.split(xy[0],2, axis=1)\n",
    "    ax2.scatter(x,y,s=80)\n",
    "    me,_  = h.encode(xy, training=False)\n",
    "    gpt = me[0].numpy()\n",
    "    ax1.scatter(gpt[0],gpt[1])\n",
    "    \n",
    "    XY = h.decode(me, training=False)[0]\n",
    "    XY = tf.sigmoid(XY)\n",
    "    X,Y = tf.split(XY[0], num_or_size_splits=2)\n",
    "    X,Y = (X.numpy(), Y.numpy())\n",
    "    ax2.scatter(X,Y,s=40)\n",
    "\n",
    "print(qsh_pos)    \n",
    "qsh_pos += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy,_ = [x for x in ds.batch(2).take(1)][0]\n",
    "me,va = h.encode(xy)\n",
    "XY = h.decode(me, apply_sigmoid=True)\n",
    "XY[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
